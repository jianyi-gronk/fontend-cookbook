## 1. BFF

#### 1.1 基础介绍

- BFF，即 Backends For Frontends ( 服务于前端的后端 )，也就是前端和后端的中间层，有利于减少前端和服务端之间的冲突，更方便前端去做数据的展示和渲染
- 主要应用场景如下：
  - API 网关
  - 服务器端渲染（ Server Side Rendering ）
  - 会话管理（ Session Management ）
  - 文件上传
  - WebSocket、Server Sent Events、Long Polling
- BFF 是 UI 的服务器，主要作用是 “显示” 相关的处理。另一方面，后端主要负责提供独立于 UI 的 “核心” 功能，称为 “领域逻辑” 或 “业务逻辑”，所以切记不能什么功能都往 BFF 上放
- BFF 并不局限于某种语言，可以用 Nodejs，也可以用 Go 等

#### 1.2 接入场景

- 无后端支持，但有后端能力诉求
  - 前端场景的项目中，无后端人力支持，并且涉及到了后端能力诉求（ 数据库存储等 ）
  - 比如维护某平台，无后端人力支持，但有数据库持久化存储等诉求
- 不适合后端支持，并且有明确收益
  - 对于一些接口聚合裁剪，RPC 转发，SSR 等，不适合后端支持，但可接入 BFF 且有收益
  - 接口聚合裁剪
    - 比如有些场景后端不支持数据编排能力，需要通过 BFF 层进行接口的聚合或裁剪能力，让前后端各自专注核心业务，让双方的协同性更高
  - RPC 转发
    - 很多时候，后端只提供 RPC 接口，不提供 http 接口，为了前端开发效率，需要 BFF 层做 RPC 转发能力
  - SSR
    - 比如需要对页面模版做一些数据注入，进行前端性能提升工作，但是后端不支持

## 2. Go 和 Node 性能对比

#### 2.1 HTTP 框架

- gulu（ Nodejs ）
  - 调用接口：GET /
  - qps：1700+
  - pct avg：5.76 ms
  - pct 50：4.17 ms
  - pct 99：30.1 ms
  - acess.log：0.05 ～ 0.08 ms
- hertz（ Go ）
  - 调用接口：GET /
  - qps：8000+
  - pct avg：1.1 ms
  - pct 50：1 ms
  - pct 99：4.71ms
  - access.log：0.04 ～ 0.07 ms

#### 2.2 HTTP 框架 + RPC Client

- gulu + gulu/rpc（ Nodejs ）
  - 调用接口：GET /rpc
  - qps：500+
  - pct avg：17.5 ms
  - pct 50：11.33 ms
  - pct 99：90.45 ms
  - access.log：5 ～ 9 ms
  - call.log：5 ～ 8 ms
- hertz + kitex（ Go ）
  - 调用接口：GET /rpc
  - 注意：延时较大，IO 消耗 > CPU 消耗，所以 CPU 未满，性能未到达上限
  - qps：1400+
  - pct avg：6.85 ms
  - pct 50：7.94 ms
  - pct 99：44.86 ms
  - access.log：3 ～ 5 ms
  - call.log：3 ～ 5 ms

#### 2.3 总结

- http 裸亚情况下 Go 的 qps 是 Nodejs 的 5 倍左右，加上 RPC 的话，是 3 倍左右，在实际业务中，如果加上很多中间件（ 日志 等 ），性能差距会更小

## 3. 抉择 BFF

- 抉择顺序如下：
  1. 如果 BFF 的诉求是有后端支持，则后端去支持，不使用 BFF
  2. 如果在 提效，降本，性能，安全，稳定性，可维护性方面中没有明确收益，则仍然不使用 BFF
  3. 如果业务组内对 Go 体系很熟悉，则选择 Go 作为 BFF 的语言
  4. 如果接口请求量超过 1w 个每秒，则还是需要选择 Go 作为 BFF 语言
  5. 最后选择 Nodejs 实现 BFF
- 评估成本：
  - 前端人力成本：主要是前端人力开发，维护成本等
  - 运维成本：需要额外的机器部署，因此有了更多的机器成本
  - 安全，服务建设成本：前端也需要在服务安全，稳定性等方面投入建设成本，服务安全包括权限，风控，数据脱敏等，稳定性包括日志和监控等的建设

## 4. API 网关

- 在 BFF 侧可以对 API 接口字段进行 修改（ 改字段名 ），过滤（ 裁剪掉某些字段，可以减少数据大小 ），拼接（ 将多个接口拼接成一个 ），这块可以考虑用 GroupGL 技术去具体实现
- 比如服务器设计 API 时会考虑客户端的使用情况
  - 在服务端根据不同的设备类型，返回不同客户端所需要的结果
  - 而在 BFF 模式，只会为所有的客户端创建通用的 API，然后创建多个 BFF 服务，一个用于 Web 前端、一个用于 iOS 移动端，另一个用于 Android 移动端等等
- 比如浏览器可以同时发出请求的次数是有限制的
  - 使用 HTTP/1.1 的时候，一次最多只能建立 6 条连接。即如果想批量请求 API，也会收到此限制。此外，每次发出请求时检查会话并建立 TCP 连接，会增加连接服务器的成本
  - 此时 BFF 可以将此 API 合并为一个请求。这样就不受浏览器请求数量的限制。此外，由于只需要进行一次会话检查和 TCP 连接，因此可以更高效的访问网络资源
